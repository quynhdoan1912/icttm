{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file will be used to contain data processing components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install boto3 pyspark delta-spark python-dotenv py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the following to set the environment variables in the notebook if you don't set manually access key, secret key and endpoint in minio\n",
    "os.environ['OBJ_STORAGE_ACCESS_KEY'] = 'ZfDdneDqDMWO07RvroVX'\n",
    "os.environ['OBJ_STORAGE_SECRET_KEY'] = 'v8V87ClJJwLAIqBPll1ix4V93Vgjdpbt0sowjt5k'\n",
    "os.environ['OBJ_STORAGE_ENDPOINT'] = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define S3 storage\n",
    "obj_storage_access_key = os.getenv('OBJ_STORAGE_ACCESS_KEY')\n",
    "obj_storage_secret_key = os.getenv('OBJ_STORAGE_SECRET_KEY')\n",
    "obj_storage_endpoint = os.getenv('OBJ_STORAGE_ENDPOINT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of data in minio\n",
    "path_1 = \"s3a://data/data-raw/data.json\"\n",
    "path_2 = \"s3a://data/data-raw/data2.json\"\n",
    "path_3 = \"s3a://data/data-raw/data3.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession with necessary configurations\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"ZfDdneDqDMWO07RvroVX\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"v8V87ClJJwLAIqBPll1ix4V93Vgjdpbt0sowjt5k\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON files into Spark DataFrames\n",
    "df_1 = spark.read.json(path_1)\n",
    "df_2 = spark.read.json(path_2)\n",
    "df_3 = spark.read.json(path_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----+------------+\n",
      "|_corrupt_record|id  |name|type        |\n",
      "+---------------+----+----+------------+\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |1001|NULL|Regular     |\n",
      "|NULL           |1002|NULL|Chocolate   |\n",
      "|NULL           |1003|NULL|Blueberry   |\n",
      "|NULL           |1004|NULL|Devil's Food|\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |NULL|NULL|NULL        |\n",
      "|NULL           |5001|NULL|None        |\n",
      "|NULL           |5002|NULL|Glazed      |\n",
      "|NULL           |5003|NULL|Chocolate   |\n",
      "|NULL           |5004|NULL|Maple       |\n",
      "|NULL           |5005|NULL|Sugar       |\n",
      "+---------------+----+----+------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---------------+----+----------------+------------------------+\n",
      "|_corrupt_record|id  |name            |type                    |\n",
      "+---------------+----+----------------+------------------------+\n",
      "|NULL           |1003|NULL            |Blueberry               |\n",
      "|NULL           |5002|NULL            |Glazed                  |\n",
      "|NULL           |NULL|NULL            |NULL                    |\n",
      "|NULL           |1004|NULL            |Devil's Food            |\n",
      "|NULL           |5007|NULL            |Powdered Sugar          |\n",
      "|NULL           |6004|Strawberry Jelly|NULL                    |\n",
      "|NULL           |1001|NULL            |Regular                 |\n",
      "|NULL           |5005|NULL            |Sugar                   |\n",
      "|NULL           |1002|NULL            |Chocolate               |\n",
      "|NULL           |5006|NULL            |Chocolate with Sprink   |\n",
      "|NULL           |1005|NULL            |Custard                 |\n",
      "|NULL           |6001|None            |NULL                    |\n",
      "|NULL           |5009|NULL            |Vanilla Cream           |\n",
      "|NULL           |5004|NULL            |Maple                   |\n",
      "|NULL           |5006|NULL            |Chocolate with Sprinkles|\n",
      "|NULL           |5008|NULL            |Strawberry              |\n",
      "|NULL           |5003|NULL            |Chocolate               |\n",
      "|NULL           |5001|NULL            |None                    |\n",
      "|NULL           |6002|Custard         |NULL                    |\n",
      "|NULL           |6003|Whipped Cream   |NULL                    |\n",
      "+---------------+----+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge the DataFrames and write the result to a JSON file\n",
    "merged_result = df_1.union(df_2).union(df_3)\n",
    "merged_result.write.format(\"json\").mode('overwrite').save(\"s3a://data/data-result/result.json\")\n",
    "\n",
    "# Read the JSON file from S3 into a DataFrame\n",
    "df_result = spark.read.json(\"s3a://data/data-result/result.json\")\n",
    "df_result.show(truncate=False)\n",
    "\n",
    "# Drop duplicate rows from the DataFrame\n",
    "df_result = df_result.dropDuplicates()\n",
    "df_result.show(truncate=False)\n",
    "\n",
    "# Write the deduplicated DataFrame to a Parquet file on S3\n",
    "df_result.write.format(\"parquet\").save(\"s3a://data/data-result/result.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
